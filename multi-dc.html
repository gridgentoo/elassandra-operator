

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Mutli-datacenter deployment &mdash; Elassandra Operator Documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://media.readthedocs.org/css/sphinx_rtd_theme.css" type="text/css" />
  <link rel="stylesheet" href="https://media.readthedocs.org/css/readthedocs-doc-embed.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/custom.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Security" href="security.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> Elassandra-Operator
          

          
            
            <img src="_static/elassandra-operator.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="helm-setup.html">HELM setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator-setup.html">Operator setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="provisioning.html">Provision a Datacenter</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced-services.html">Advanced Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="operations.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Mutli-datacenter deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#kubernetes-cluster-setup">Kubernetes cluster setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#aks">AKS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#aks-storageclass">AKS StorageClass</a></li>
<li class="toctree-l4"><a class="reference internal" href="#aks-firewall-rules">AKS Firewall rules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#gke">GKE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#coredns-installation">CoreDNS installation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gke-storageclass">GKE StorageClass</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gke-firewall-rules">GKE Firewall rules</a></li>
<li class="toctree-l4"><a class="reference internal" href="#webhook-in-gke-private-cluster">Webhook in GKE private cluster</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#aws">AWS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-operators">Deploy operators</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#externaldns">ExternalDNS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#coredns">CoreDNS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#traefik">Traefik</a></li>
<li class="toctree-l3"><a class="reference internal" href="#elassandra-operator">Elassandra Operator</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#multi-datacenter-setup">Multi-datacenter setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#deploy-dc1-on-kube1">Deploy dc1 on kube1</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploy-dc2-on-kube2">Deploy dc2 on kube2</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#cleaning-up">Cleaning up</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Elassandra-Operator</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Mutli-datacenter deployment</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/multi-dc.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mutli-datacenter-deployment">
<h1>Mutli-datacenter deployment<a class="headerlink" href="#mutli-datacenter-deployment" title="Permalink to this headline">¶</a></h1>
<p>With the Elassandra operator, you can connect Elassandra datacenter running in the same or distinct Kubernetes clusters.
The following chapter explains how to setup an Elassandra multi-datacenter deployment over the internet.</p>
<div class="section" id="kubernetes-cluster-setup">
<h2>Kubernetes cluster setup<a class="headerlink" href="#kubernetes-cluster-setup" title="Permalink to this headline">¶</a></h2>
<p>Here is instruction to prepare your Kubernetes cluster before deploying the Elassandra stack.</p>
<div class="section" id="aks">
<h3>AKS<a class="headerlink" href="#aks" title="Permalink to this headline">¶</a></h3>
<p>In order to create your Azure Kubernetes cluster, see the <a class="reference external" href="https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough">Azure Quickstart</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To setup an AKS cluster with having a public IP address on each Kubernetes nodes, you need to install the following <a class="reference external" href="https://docs.microsoft.com/en-us/azure/aks/use-multiple-node-pools#assign-a-public-ip-per-node-for-your-node-pools-preview">Azure preview features</a>:</p>
<p>az extension add –name aks-preview
az extension update –name aks-preview
az feature register –name NodePublicIPPreview –namespace Microsoft.ContainerService</p>
</div>
<p>Create a regional AKS cluster with the Azure network plugin and a default nodepool based
on a <a class="reference external" href="https://docs.microsoft.com/en-us/rest/api/compute/virtualmachinescalesets">VirtualMachineScaleSets</a> that assigns
a public IP address to each virtual machine:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>az aks create --name &quot;${K8S_CLUSTER_NAME}&quot; \
              --resource-group ${RESOURCE_GROUP_NAME} \
              --network-plugin azure \
              --node-count 3 \
              --node-vm-size Standard_D2_v3 \
              --vm-set-type VirtualMachineScaleSets \
              --output table \
              --zone 1 2 3 \
              --enable-node-public-ip
az aks get-credentials --name &quot;${K8S_CLUSTER_NAME}&quot; --resource-group $RESOURCE_GROUP_NAME --output table
</pre></div>
</div>
<p>Unfortunately, AKS does not map VM’s public IP address to the Kubernetes node external IP address, so the trick is to add these public IP addresses as a
kubernetes custom label <code class="docutils literal notranslate"><span class="pre">kubernetes.strapdata.com/public-ip</span></code> to each nodes, here for the first Kubernetes node in our AKS cluster:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>add_vmss_public_ip() {
   AKS_RG_NAME=$(az resource show --namespace Microsoft.ContainerService --resource-type managedClusters -g $RESOURCE_GROUP_NAME -n $K8S_CLUSTER_NAME | jq -r .properties.nodeResourceGroup)
   AKS_VMSS_INSTANCE=$(kubectl get nodes -o json | jq -r &quot;.items[${1:-0}].metadata.name&quot;)
   PUBLIC_IP=$(az vmss list-instance-public-ips -g $AKS_RG_NAME -n ${AKS_VMSS_INSTANCE::-6} | jq -r &quot;.[${1:-0}].ipAddress&quot;)
   kubectl label nodes --overwrite $AKS_VMSS_INSTANCE kubernetes.strapdata.com/public-ip=$PUBLIC_IP
}

add_vmss_public_ip 0
add_vmss_public_ip 1
add_vmss_public_ip 2
</pre></div>
</div>
<p>As the result, you should have kubernetes nodes properly labeled with zone and public-ip:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">get</span> <span class="n">nodes</span> <span class="o">-</span><span class="n">o</span> <span class="n">wide</span> <span class="o">-</span><span class="n">L</span> <span class="n">failure</span><span class="o">-</span><span class="n">domain</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">kubernetes</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">zone</span><span class="p">,</span><span class="n">kubernetes</span><span class="o">.</span><span class="n">strapdata</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">public</span><span class="o">-</span><span class="n">ip</span>
<span class="n">NAME</span>                                <span class="n">STATUS</span>   <span class="n">ROLES</span>   <span class="n">AGE</span>     <span class="n">VERSION</span>    <span class="n">INTERNAL</span><span class="o">-</span><span class="n">IP</span>   <span class="n">EXTERNAL</span><span class="o">-</span><span class="n">IP</span>   <span class="n">OS</span><span class="o">-</span><span class="n">IMAGE</span>             <span class="n">KERNEL</span><span class="o">-</span><span class="n">VERSION</span>      <span class="n">CONTAINER</span><span class="o">-</span><span class="n">RUNTIME</span>       <span class="n">ZONE</span>            <span class="n">PUBLIC</span><span class="o">-</span><span class="n">IP</span>
<span class="n">aks</span><span class="o">-</span><span class="n">nodepool1</span><span class="o">-</span><span class="mi">32762597</span><span class="o">-</span><span class="n">vmss000000</span>   <span class="n">Ready</span>    <span class="n">agent</span>   <span class="mi">2</span><span class="n">d20h</span>   <span class="n">v1</span><span class="o">.</span><span class="mf">15.11</span>   <span class="mf">10.240</span><span class="o">.</span><span class="mf">0.4</span>    <span class="o">&lt;</span><span class="n">none</span><span class="o">&gt;</span>        <span class="n">Ubuntu</span> <span class="mf">16.04</span><span class="o">.</span><span class="mi">6</span> <span class="n">LTS</span>   <span class="mf">4.15</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="mi">1083</span><span class="o">-</span><span class="n">azure</span>   <span class="n">docker</span><span class="p">:</span><span class="o">//</span><span class="mf">3.0</span><span class="o">.</span><span class="mi">10</span><span class="o">+</span><span class="n">azure</span>   <span class="n">northeurope</span><span class="o">-</span><span class="mi">1</span>   <span class="mf">20.54</span><span class="o">.</span><span class="mf">72.64</span>
<span class="n">aks</span><span class="o">-</span><span class="n">nodepool1</span><span class="o">-</span><span class="mi">32762597</span><span class="o">-</span><span class="n">vmss000001</span>   <span class="n">Ready</span>    <span class="n">agent</span>   <span class="mi">2</span><span class="n">m32s</span>   <span class="n">v1</span><span class="o">.</span><span class="mf">15.11</span>   <span class="mf">10.240</span><span class="o">.</span><span class="mf">0.35</span>   <span class="o">&lt;</span><span class="n">none</span><span class="o">&gt;</span>        <span class="n">Ubuntu</span> <span class="mf">16.04</span><span class="o">.</span><span class="mi">6</span> <span class="n">LTS</span>   <span class="mf">4.15</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="mi">1083</span><span class="o">-</span><span class="n">azure</span>   <span class="n">docker</span><span class="p">:</span><span class="o">//</span><span class="mf">3.0</span><span class="o">.</span><span class="mi">10</span><span class="o">+</span><span class="n">azure</span>   <span class="n">northeurope</span><span class="o">-</span><span class="mi">2</span>   <span class="mf">40.113</span><span class="o">.</span><span class="mf">33.9</span>
<span class="n">aks</span><span class="o">-</span><span class="n">nodepool1</span><span class="o">-</span><span class="mi">32762597</span><span class="o">-</span><span class="n">vmss000002</span>   <span class="n">Ready</span>    <span class="n">agent</span>   <span class="mi">2</span><span class="n">m29s</span>   <span class="n">v1</span><span class="o">.</span><span class="mf">15.11</span>   <span class="mf">10.240</span><span class="o">.</span><span class="mf">0.66</span>   <span class="o">&lt;</span><span class="n">none</span><span class="o">&gt;</span>        <span class="n">Ubuntu</span> <span class="mf">16.04</span><span class="o">.</span><span class="mi">6</span> <span class="n">LTS</span>   <span class="mf">4.15</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="mi">1083</span><span class="o">-</span><span class="n">azure</span>   <span class="n">docker</span><span class="p">:</span><span class="o">//</span><span class="mf">3.0</span><span class="o">.</span><span class="mi">10</span><span class="o">+</span><span class="n">azure</span>   <span class="n">northeurope</span><span class="o">-</span><span class="mi">3</span>   <span class="mf">20.54</span><span class="o">.</span><span class="mf">80.104</span>
</pre></div>
</div>
<div class="section" id="aks-storageclass">
<h4>AKS StorageClass<a class="headerlink" href="#aks-storageclass" title="Permalink to this headline">¶</a></h4>
<p>Azure persistent volumes are bound to an availability zone, so we need to defined one storageClass per zone in our Kubernetes cluster,
and each Elassandra rack or statefulSet will be bound to the corresponding storageClass.
This is done here using the HELM chart strapdata/storageclass.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>for z in northeurope-1 northeurope-2 northeurope-3; do
    helm install --name ssd-$z --namespace kube-system \
        --set parameters.kind=&quot;Managed&quot; \
        --set parameters.cachingmode=&quot;ReadOnly&quot; \
        --set parameters.storageaccounttype=&quot;StandardSSD_LRS&quot; \
        --set provisioner=&quot;kubernetes.io/azure-disk&quot; \
        --set zone=&quot;${z}&quot; \
        --set nameOverride=&quot;ssd-$z&quot; \
        $HELM_REPO/storageclass
done
</pre></div>
</div>
</div>
<div class="section" id="aks-firewall-rules">
<h4>AKS Firewall rules<a class="headerlink" href="#aks-firewall-rules" title="Permalink to this headline">¶</a></h4>
<p>Finally, you may need to authorize inbound Elassandra connections on the following TCP ports:</p>
<ul class="simple">
<li><p>Cassandra storage port (usually 7000 or 7001) for internode connections</p></li>
<li><p>Cassandra native CQL port (usually 9042) for client to node connections.</p></li>
<li><p>Elasticsearch HTTP port (usually 9200) for the Elasticsearch REST API.</p></li>
</ul>
<p>Assuming you deploy an Elassandra datacenter respectively using ports 39000, 39001, and 39002 exposed to the internet, with no source IP address restrictions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>AKS_RG_NAME=$(az resource show --namespace Microsoft.ContainerService --resource-type managedClusters -g $RESOURCE_GROUP_NAME -n &quot;${K8S_CLUSTER_NAME}&quot; | jq -r .properties.nodeResourceGroup)
NSG_NAME=$(az network nsg list -g $AKS_RG_NAME | jq -r .[0].name)
az network nsg rule create \
    --resource-group $AKS_RG_NAME \
    --nsg-name $NSG_NAME \
    --name elassandra_inbound \
    --description &quot;Elassandra inbound rule&quot; \
    --priority 2000 \
    --access Allow \
    --source-address-prefixes Internet \
    --protocol Tcp \
    --direction Inbound \
    --destination-address-prefixes &#39;*&#39; \
    --destination-port-ranges 39000-39002
</pre></div>
</div>
<p>Your Kubernetes cluster is now ready to deploy an Elassandra datacenter accessible from the internet world.</p>
</div>
</div>
<div class="section" id="gke">
<h3>GKE<a class="headerlink" href="#gke" title="Permalink to this headline">¶</a></h3>
<p>Create a <a class="reference external" href="https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-regional-cluster">Regional Kubernetes cluster</a> on GCP:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud container clusters create $K8S_CLUSTER_NAME \
  --region $GCLOUD_REGION \
  --project $GCLOUD_PROJECT \
  --machine-type &quot;n1-standard-2&quot; \
  --cluster-version=1.15 \
  --tags=$K8S_CLUSTER_NAME \
  --num-nodes &quot;1&quot;
gcloud container clusters get-credentials $K8S_CLUSTER_NAME --region $GCLOUD_REGION --project $GCLOUD_PROJECT
</pre></div>
</div>
<p>Enable RBAC:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $(gcloud config get-value account)
</pre></div>
</div>
<div class="section" id="coredns-installation">
<h4>CoreDNS installation<a class="headerlink" href="#coredns-installation" title="Permalink to this headline">¶</a></h4>
<p>GKE is provided with KubeDNS by default, which does not allows to configure host aliases required to run Cassandra Reaper with an AddressTranslator.
So we need to install CoreDNS configured to import custom configuration (see <a class="reference external" href="https://coredns.io/plugins/import/">CoreDNS import plugin</a>),
and configure KubeDNS stub domains to forward to CoreDNS.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">helm</span> <span class="n">install</span> <span class="o">--</span><span class="n">name</span> <span class="n">coredns</span> <span class="o">--</span><span class="n">namespace</span><span class="o">=</span><span class="n">kube</span><span class="o">-</span><span class="n">system</span> <span class="o">-</span><span class="n">f</span> <span class="n">integ</span><span class="o">-</span><span class="n">test</span><span class="o">/</span><span class="n">gke</span><span class="o">/</span><span class="n">coredns</span><span class="o">-</span><span class="n">values</span><span class="o">.</span><span class="n">yaml</span> <span class="n">stable</span><span class="o">/</span><span class="n">coredns</span>
</pre></div>
</div>
<p>Where coredns-values.yaml is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Default values for coredns.</span>
<span class="c1"># This is a YAML-formatted file.</span>
<span class="c1"># Declare variables to be passed into your templates.</span>

<span class="n">image</span><span class="p">:</span>
  <span class="n">repository</span><span class="p">:</span> <span class="n">coredns</span><span class="o">/</span><span class="n">coredns</span>
  <span class="n">tag</span><span class="p">:</span> <span class="s2">&quot;1.6.9&quot;</span>
  <span class="n">pullPolicy</span><span class="p">:</span> <span class="n">IfNotPresent</span>

<span class="n">replicaCount</span><span class="p">:</span> <span class="mi">1</span>

<span class="n">resources</span><span class="p">:</span>
  <span class="n">limits</span><span class="p">:</span>
    <span class="n">cpu</span><span class="p">:</span> <span class="mi">100</span><span class="n">m</span>
    <span class="n">memory</span><span class="p">:</span> <span class="mi">128</span><span class="n">Mi</span>
  <span class="n">requests</span><span class="p">:</span>
    <span class="n">cpu</span><span class="p">:</span> <span class="mi">100</span><span class="n">m</span>
    <span class="n">memory</span><span class="p">:</span> <span class="mi">128</span><span class="n">Mi</span>

<span class="n">serviceType</span><span class="p">:</span> <span class="s2">&quot;ClusterIP&quot;</span>

<span class="n">prometheus</span><span class="p">:</span>
  <span class="n">service</span><span class="p">:</span>
    <span class="n">enabled</span><span class="p">:</span> <span class="n">false</span>
    <span class="n">annotations</span><span class="p">:</span>
      <span class="n">prometheus</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">scrape</span><span class="p">:</span> <span class="s2">&quot;true&quot;</span>
      <span class="n">prometheus</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">port</span><span class="p">:</span> <span class="s2">&quot;9153&quot;</span>
  <span class="n">monitor</span><span class="p">:</span>
    <span class="n">enabled</span><span class="p">:</span> <span class="n">false</span>
    <span class="n">additionalLabels</span><span class="p">:</span> <span class="p">{}</span>
    <span class="n">namespace</span><span class="p">:</span> <span class="s2">&quot;&quot;</span>

<span class="n">service</span><span class="p">:</span>
  <span class="c1"># clusterIP: &quot;&quot;</span>
  <span class="c1"># loadBalancerIP: &quot;&quot;</span>
  <span class="c1"># externalTrafficPolicy: &quot;&quot;</span>
  <span class="n">annotations</span><span class="p">:</span>
    <span class="n">prometheus</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">scrape</span><span class="p">:</span> <span class="s2">&quot;true&quot;</span>
    <span class="n">prometheus</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">port</span><span class="p">:</span> <span class="s2">&quot;9153&quot;</span>

<span class="n">serviceAccount</span><span class="p">:</span>
  <span class="n">create</span><span class="p">:</span> <span class="n">false</span>
  <span class="c1"># The name of the ServiceAccount to use</span>
  <span class="c1"># If not set and create is true, a name is generated using the fullname template</span>
  <span class="n">name</span><span class="p">:</span>

<span class="n">rbac</span><span class="p">:</span>
  <span class="c1"># If true, create &amp; use RBAC resources</span>
  <span class="n">create</span><span class="p">:</span> <span class="n">true</span>
  <span class="c1"># If true, create and use PodSecurityPolicy</span>
  <span class="n">pspEnable</span><span class="p">:</span> <span class="n">false</span>
  <span class="c1"># The name of the ServiceAccount to use.</span>
  <span class="c1"># If not set and create is true, a name is generated using the fullname template</span>
  <span class="c1"># name:</span>

<span class="c1"># isClusterService specifies whether chart should be deployed as cluster-service or normal k8s app.</span>
<span class="n">isClusterService</span><span class="p">:</span> <span class="n">true</span>

<span class="c1"># Optional priority class to be used for the coredns pods. Used for autoscaler if autoscaler.priorityClassName not set.</span>
<span class="n">priorityClassName</span><span class="p">:</span> <span class="s2">&quot;&quot;</span>

<span class="c1"># Default zone is what Kubernetes recommends:</span>
<span class="c1"># https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#coredns-configmap-options</span>
<span class="n">servers</span><span class="p">:</span>
  <span class="o">-</span> <span class="n">zones</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">zone</span><span class="p">:</span> <span class="o">.</span>
    <span class="n">port</span><span class="p">:</span> <span class="mi">53</span>
    <span class="n">plugins</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">errors</span>
      <span class="c1"># Serves a /health endpoint on :8080, required for livenessProbe</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">health</span>
        <span class="n">configBlock</span><span class="p">:</span> <span class="o">|-</span>
          <span class="n">lameduck</span> <span class="mi">5</span><span class="n">s</span>
      <span class="c1"># Serves a /ready endpoint on :8181, required for readinessProbe</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">ready</span>
      <span class="c1"># Required to query kubernetes API for data</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">kubernetes</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="n">cluster</span><span class="o">.</span><span class="n">local</span> <span class="ow">in</span><span class="o">-</span><span class="n">addr</span><span class="o">.</span><span class="n">arpa</span> <span class="n">ip6</span><span class="o">.</span><span class="n">arpa</span>
        <span class="n">configBlock</span><span class="p">:</span> <span class="o">|-</span>
          <span class="n">pods</span> <span class="n">insecure</span>
          <span class="n">fallthrough</span> <span class="ow">in</span><span class="o">-</span><span class="n">addr</span><span class="o">.</span><span class="n">arpa</span> <span class="n">ip6</span><span class="o">.</span><span class="n">arpa</span>
          <span class="n">ttl</span> <span class="mi">30</span>
      <span class="c1"># Serves a /metrics endpoint on :9153, required for serviceMonitor</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">prometheus</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="mf">0.0</span><span class="o">.</span><span class="mf">0.0</span><span class="p">:</span><span class="mi">9153</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">forward</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="o">.</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">resolv</span><span class="o">.</span><span class="n">conf</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">cache</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="mi">30</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">loop</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">reload</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">loadbalance</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="kn">import</span>
        <span class="nn">parameters</span><span class="p">:</span> <span class="s2">&quot;custom/*.override&quot;</span>

<span class="c1"># Complete example with all the options:</span>
<span class="c1"># - zones:                 # the `zones` block can be left out entirely, defaults to &quot;.&quot;</span>
<span class="c1">#   - zone: hello.world.   # optional, defaults to &quot;.&quot;</span>
<span class="c1">#     scheme: tls://       # optional, defaults to &quot;&quot; (which equals &quot;dns://&quot; in CoreDNS)</span>
<span class="c1">#   - zone: foo.bar.</span>
<span class="c1">#     scheme: dns://</span>
<span class="c1">#     use_tcp: true        # set this parameter to optionally expose the port on tcp as well as udp for the DNS protocol</span>
<span class="c1">#                          # Note that this will not work if you are also exposing tls or grpc on the same server</span>
<span class="c1">#   port: 12345            # optional, defaults to &quot;&quot; (which equals 53 in CoreDNS)</span>
<span class="c1">#   plugins:               # the plugins to use for this server block</span>
<span class="c1">#   - name: kubernetes     # name of plugin, if used multiple times ensure that the plugin supports it!</span>
<span class="c1">#     parameters: foo bar  # list of parameters after the plugin</span>
<span class="c1">#     configBlock: |-      # if the plugin supports extra block style config, supply it here</span>
<span class="c1">#       hello world</span>
<span class="c1">#       foo bar</span>

<span class="c1"># expects input structure as per specification https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#affinity-v1-core</span>
<span class="c1"># for example:</span>
<span class="c1">#   affinity:</span>
<span class="c1">#     nodeAffinity:</span>
<span class="c1">#      requiredDuringSchedulingIgnoredDuringExecution:</span>
<span class="c1">#        nodeSelectorTerms:</span>
<span class="c1">#        - matchExpressions:</span>
<span class="c1">#          - key: foo.bar.com/role</span>
<span class="c1">#            operator: In</span>
<span class="c1">#            values:</span>
<span class="c1">#            - master</span>
<span class="n">affinity</span><span class="p">:</span> <span class="p">{}</span>

<span class="c1"># Node labels for pod assignment</span>
<span class="c1"># Ref: https://kubernetes.io/docs/user-guide/node-selection/</span>
<span class="n">nodeSelector</span><span class="p">:</span> <span class="p">{}</span>

<span class="c1"># expects input structure as per specification https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#toleration-v1-core</span>
<span class="c1"># for example:</span>
<span class="c1">#   tolerations:</span>
<span class="c1">#   - key: foo.bar.com/role</span>
<span class="c1">#     operator: Equal</span>
<span class="c1">#     value: master</span>
<span class="c1">#     effect: NoSchedule</span>
<span class="n">tolerations</span><span class="p">:</span> <span class="p">[]</span>

<span class="c1"># https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget</span>
<span class="n">podDisruptionBudget</span><span class="p">:</span> <span class="p">{}</span>

<span class="c1"># configure custom zone files as per https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/</span>
<span class="n">zoneFiles</span><span class="p">:</span> <span class="p">[]</span>
<span class="c1">#  - filename: example.db</span>
<span class="c1">#    domain: example.com</span>
<span class="c1">#    contents: |</span>
<span class="c1">#      example.com.   IN SOA sns.dns.icann.com. noc.dns.icann.com. 2015082541 7200 3600 1209600 3600</span>
<span class="c1">#      example.com.   IN NS  b.iana-servers.net.</span>
<span class="c1">#      example.com.   IN NS  a.iana-servers.net.</span>
<span class="c1">#      example.com.   IN A   192.168.99.102</span>
<span class="c1">#      *.example.com. IN A   192.168.99.102</span>

<span class="c1"># optional array of extra volumes to create</span>
<span class="n">extraVolumes</span><span class="p">:</span>
  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">custom</span><span class="o">-</span><span class="n">config</span><span class="o">-</span><span class="n">volume</span>
    <span class="n">configMap</span><span class="p">:</span>
      <span class="n">name</span><span class="p">:</span> <span class="n">coredns</span><span class="o">-</span><span class="n">custom</span>
<span class="c1"># - name: some-volume-name</span>
<span class="c1">#   emptyDir: {}</span>
<span class="c1"># optional array of mount points for extraVolumes</span>
<span class="n">extraVolumeMounts</span><span class="p">:</span>
  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">custom</span><span class="o">-</span><span class="n">config</span><span class="o">-</span><span class="n">volume</span>
    <span class="n">mountPath</span><span class="p">:</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">coredns</span><span class="o">/</span><span class="n">custom</span>
<span class="c1"># - name: some-volume-name</span>
<span class="c1">#   mountPath: /etc/wherever</span>

<span class="c1"># optional array of secrets to mount inside coredns container</span>
<span class="c1"># possible usecase: need for secure connection with etcd backend</span>
<span class="n">extraSecrets</span><span class="p">:</span> <span class="p">[]</span>
<span class="c1"># - name: etcd-client-certs</span>
<span class="c1">#   mountPath: /etc/coredns/tls/etcd</span>
<span class="c1"># - name: some-fancy-secret</span>
<span class="c1">#   mountPath: /etc/wherever</span>

<span class="c1"># Custom labels to apply to Deployment, Pod, Service, ServiceMonitor. Including autoscaler if enabled.</span>
<span class="n">customLabels</span><span class="p">:</span> <span class="p">{}</span>

<span class="c1">## Configue a cluster-proportional-autoscaler for coredns</span>
<span class="c1"># See https://github.com/kubernetes-incubator/cluster-proportional-autoscaler</span>
<span class="n">autoscaler</span><span class="p">:</span>
  <span class="c1"># Enabled the cluster-proportional-autoscaler</span>
  <span class="n">enabled</span><span class="p">:</span> <span class="n">false</span>

  <span class="c1"># Number of cores in the cluster per coredns replica</span>
  <span class="n">coresPerReplica</span><span class="p">:</span> <span class="mi">256</span>
  <span class="c1"># Number of nodes in the cluster per coredns replica</span>
  <span class="n">nodesPerReplica</span><span class="p">:</span> <span class="mi">16</span>
  <span class="c1"># Min size of replicaCount</span>
  <span class="nb">min</span><span class="p">:</span> <span class="mi">0</span>
  <span class="c1"># Max size of replicaCount (default of 0 is no max)</span>
  <span class="nb">max</span><span class="p">:</span> <span class="mi">0</span>
  <span class="c1"># Whether to include unschedulable nodes in the nodes/cores calculations - this requires version 1.8.0+ of the autoscaler</span>
  <span class="n">includeUnschedulableNodes</span><span class="p">:</span> <span class="n">false</span>
  <span class="c1"># If true does not allow single points of failure to form</span>
  <span class="n">preventSinglePointFailure</span><span class="p">:</span> <span class="n">true</span>

  <span class="n">image</span><span class="p">:</span>
    <span class="n">repository</span><span class="p">:</span> <span class="n">k8s</span><span class="o">.</span><span class="n">gcr</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">cluster</span><span class="o">-</span><span class="n">proportional</span><span class="o">-</span><span class="n">autoscaler</span><span class="o">-</span><span class="n">amd64</span>
    <span class="n">tag</span><span class="p">:</span> <span class="s2">&quot;1.8.0&quot;</span>
    <span class="n">pullPolicy</span><span class="p">:</span> <span class="n">IfNotPresent</span>

  <span class="c1"># Optional priority class to be used for the autoscaler pods. priorityClassName used if not set.</span>
  <span class="n">priorityClassName</span><span class="p">:</span> <span class="s2">&quot;&quot;</span>

  <span class="c1"># expects input structure as per specification https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#affinity-v1-core</span>
  <span class="n">affinity</span><span class="p">:</span> <span class="p">{}</span>

  <span class="c1"># Node labels for pod assignment</span>
  <span class="c1"># Ref: https://kubernetes.io/docs/user-guide/node-selection/</span>
  <span class="n">nodeSelector</span><span class="p">:</span> <span class="p">{}</span>

  <span class="c1"># expects input structure as per specification https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#toleration-v1-core</span>
  <span class="n">tolerations</span><span class="p">:</span> <span class="p">[]</span>

  <span class="c1"># resources for autoscaler pod</span>
  <span class="n">resources</span><span class="p">:</span>
    <span class="n">requests</span><span class="p">:</span>
      <span class="n">cpu</span><span class="p">:</span> <span class="s2">&quot;20m&quot;</span>
      <span class="n">memory</span><span class="p">:</span> <span class="s2">&quot;10Mi&quot;</span>
    <span class="n">limits</span><span class="p">:</span>
      <span class="n">cpu</span><span class="p">:</span> <span class="s2">&quot;20m&quot;</span>
      <span class="n">memory</span><span class="p">:</span> <span class="s2">&quot;10Mi&quot;</span>

  <span class="c1"># Options for autoscaler configmap</span>
  <span class="n">configmap</span><span class="p">:</span>
    <span class="c1">## Annotations for the coredns-autoscaler configmap</span>
    <span class="c1"># i.e. strategy.spinnaker.io/versioned: &quot;false&quot; to ensure configmap isn&#39;t renamed</span>
    <span class="n">annotations</span><span class="p">:</span> <span class="p">{}</span>
</pre></div>
</div>
<p>Once CoreDNS is installed, we need to add a KubeDNS a stub domain to forward request for domain <strong>internal.strapdata.com</strong>
to the CoreDNS service, and restart KubeDNS pods.
The <strong>internal.strapdata.com</strong> is just a dummy DNS domain used to resolv public IP addresses to Kubernetes nodes internal IP addresses.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>COREDNS_SERVICE_IP=$(kubectl get  service -l k8s-app=coredns  -n kube-system -o jsonpath=&#39;{.items[0].spec.clusterIP}&#39;)
KUBEDNS_STUB_DOMAINS=&quot;{\\\&quot;internal.strapdata.com\\\&quot;: [\\\&quot;$COREDNS_SERVICE_IP\\\&quot;]}&quot;
kubectl patch configmap/kube-dns -n kube-system -p &quot;{\&quot;data\&quot;: {\&quot;stubDomains\&quot;: \&quot;$KUBEDNS_STUB_DOMAINS\&quot;}}&quot;
kubectl delete pod -l k8s-app=coredns -n kube-system
</pre></div>
</div>
</div>
<div class="section" id="gke-storageclass">
<h4>GKE StorageClass<a class="headerlink" href="#gke-storageclass" title="Permalink to this headline">¶</a></h4>
<p>Google cloud persistent volumes are bound to an availability zone, so we need to defined one storageClass per zone in our Kubernetes cluster,
and each Elassandra rack or statefulSet will be bound to the corresponding storageClass.
This is done here using the HELM chart strapdata/storageclass.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">helm</span> <span class="n">install</span> <span class="o">--</span><span class="n">name</span> <span class="n">ssd</span><span class="o">-</span><span class="n">europe</span><span class="o">-</span><span class="n">west1</span><span class="o">-</span><span class="n">b</span> <span class="o">--</span><span class="n">namespace</span> <span class="n">kube</span><span class="o">-</span><span class="n">system</span> <span class="o">--</span><span class="nb">set</span> <span class="n">zone</span><span class="o">=</span><span class="n">europe</span><span class="o">-</span><span class="n">west1</span><span class="o">-</span><span class="n">b</span><span class="p">,</span><span class="n">nameOverride</span><span class="o">=</span><span class="n">ssd</span><span class="o">-</span><span class="n">europe</span><span class="o">-</span><span class="n">west1</span><span class="o">-</span><span class="n">b</span> <span class="n">strapdata</span><span class="o">/</span><span class="n">storageclass</span>
<span class="n">helm</span> <span class="n">install</span> <span class="o">--</span><span class="n">name</span> <span class="n">ssd</span><span class="o">-</span><span class="n">europe</span><span class="o">-</span><span class="n">west1</span><span class="o">-</span><span class="n">c</span> <span class="o">--</span><span class="n">namespace</span> <span class="n">kube</span><span class="o">-</span><span class="n">system</span> <span class="o">--</span><span class="nb">set</span> <span class="n">zone</span><span class="o">=</span><span class="n">europe</span><span class="o">-</span><span class="n">west1</span><span class="o">-</span><span class="n">c</span><span class="p">,</span><span class="n">nameOverride</span><span class="o">=</span><span class="n">ssd</span><span class="o">-</span><span class="n">europe</span><span class="o">-</span><span class="n">west1</span><span class="o">-</span><span class="n">c</span> <span class="n">strapdata</span><span class="o">/</span><span class="n">storageclass</span>
<span class="n">helm</span> <span class="n">install</span> <span class="o">--</span><span class="n">name</span> <span class="n">ssd</span><span class="o">-</span><span class="n">europe</span><span class="o">-</span><span class="n">west1</span><span class="o">-</span><span class="n">d</span> <span class="o">--</span><span class="n">namespace</span> <span class="n">kube</span><span class="o">-</span><span class="n">system</span> <span class="o">--</span><span class="nb">set</span> <span class="n">zone</span><span class="o">=</span><span class="n">europe</span><span class="o">-</span><span class="n">west1</span><span class="o">-</span><span class="n">d</span><span class="p">,</span><span class="n">nameOverride</span><span class="o">=</span><span class="n">ssd</span><span class="o">-</span><span class="n">europe</span><span class="o">-</span><span class="n">west1</span><span class="o">-</span><span class="n">d</span> <span class="n">strapdata</span><span class="o">/</span><span class="n">storageclass</span>
</pre></div>
</div>
</div>
<div class="section" id="gke-firewall-rules">
<h4>GKE Firewall rules<a class="headerlink" href="#gke-firewall-rules" title="Permalink to this headline">¶</a></h4>
<p>Finally, you may need to authorize inbound Elassandra connections on the following TCP ports:</p>
<ul class="simple">
<li><p>Cassandra storage port (usually 7000 or 7001) for internode connections</p></li>
<li><p>Cassandra native CQL port (usually 9042) for client to node connections.</p></li>
<li><p>Elasticsearch HTTP port (usually 9200) for the Elasticsearch REST API.</p></li>
</ul>
<p>Assuming you deploy an Elassandra datacenter respectively using ports 39000, 39001, and 39002 exposed to the internet, with no source IP address restrictions,
and Kubernetes nodes are properly tagged:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>VPC_NETWORK=$(gcloud container clusters describe $K8S_CLUSTER_NAME --region $GCLOUD_REGION --format=&#39;value(network)&#39;)
NODE_POOLS_TARGET_TAGS=$(gcloud container clusters describe $K8S_CLUSTER_NAME --region $GCLOUD_REGION --format=&#39;value[terminator=&quot;,&quot;](nodePools.config.tags)&#39; --flatten=&#39;nodePools[].config.tags[]&#39; | sed &#39;s/,\{2,\}//g&#39;)
gcloud compute firewall-rules create &quot;allow-elassandra-inbound&quot; \
  --allow tcp:39000-39002 \
  --network=&quot;$VPC_NETWORK&quot; \
  --target-tags=&quot;$NODE_POOLS_TARGET_TAGS&quot; \
  --description=&quot;Allow elassandra inbound&quot; \
  --direction INGRESS
</pre></div>
</div>
</div>
<div class="section" id="webhook-in-gke-private-cluster">
<h4>Webhook in GKE private cluster<a class="headerlink" href="#webhook-in-gke-private-cluster" title="Permalink to this headline">¶</a></h4>
<p>When Google configure the control plane for <strong>private clusters</strong>, they automatically configure VPC peering between your
Kubernetes cluster’s network and a separate Google managed project. In order to restrict what Google are able to access within your cluster,
the firewall rules configured restrict access to your Kubernetes pods. This means that in order to use the webhook component
with a GKE private cluster, you must configure an additional firewall rule to allow the GKE control plane access to your webhook pod.</p>
<p>You can read more information on how to add firewall rules for the GKE control plane nodes in the GKE docs.
Alternatively, you can disable the hooks by setting webhookEnabled=false in your datacenter spec.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>VPC_NETWORK=$(gcloud container clusters describe $K8S_CLUSTER_NAME --region $GCLOUD_REGION --format=&#39;value(network)&#39;)
MASTER_IPV4_CIDR_BLOCK=$(gcloud container clusters describe $K8S_CLUSTER_NAME --region $GCLOUD_REGION --format=&#39;value(clusterIpv4Cidr)&#39;)
NODE_POOLS_TARGET_TAGS=$(gcloud container clusters describe $K8S_CLUSTER_NAME --region $GCLOUD_REGION --format=&#39;value[terminator=&quot;,&quot;](nodePools.config.tags)&#39; --flatten=&#39;nodePools[].config.tags[]&#39; | sed &#39;s/,\{2,\}//g&#39;)

gcloud compute firewall-rules create &quot;allow-apiserver-to-admission-webhook-443&quot; \
  --allow tcp:8443 \
  --network=&quot;$VPC_NETWORK&quot; \
  --source-ranges=&quot;$MASTER_IPV4_CIDR_BLOCK&quot; \
  --target-tags=&quot;$NODE_POOLS_TARGET_TAGS&quot; \
  --description=&quot;Allow apiserver access to admission webhook pod on port 443&quot; \
  --direction INGRESS
</pre></div>
</div>
</div>
</div>
<div class="section" id="aws">
<h3>AWS<a class="headerlink" href="#aws" title="Permalink to this headline">¶</a></h3>
<p>Coming soon…</p>
</div>
</div>
<div class="section" id="deploy-operators">
<h2>Deploy operators<a class="headerlink" href="#deploy-operators" title="Permalink to this headline">¶</a></h2>
<div class="section" id="externaldns">
<h3>ExternalDNS<a class="headerlink" href="#externaldns" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="https://github.com/kubernetes-sigs/external-dns">ExternalDNS</a> is used to automatically update your DNS zone and
create an A record for the Cassandra broadcast IP addresses. You can use it with a public or a private DNS zone.</p>
<p>In the following setup, we will use a DNS zone hosted on Azure, but you can use any other DNS provider supported by External DNS.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>helm install --name my-externaldns --namespace default \
    --set logLevel=&quot;debug&quot; \
    --set rbac.create=true \
    --set policy=&quot;sync&quot;,txtPrefix=$(kubectl config current-context)\
    --set sources[0]=&quot;service&quot;,sources[1]=&quot;ingress&quot;,sources[2]=&quot;crd&quot; \
    --set crd.create=true,crd.apiversion=&quot;externaldns.k8s.io/v1alpha1&quot;,crd.kind=&quot;DNSEndpoint&quot; \
    --set provider=&quot;azure&quot; \
    --set azure.secretName=&quot;$AZURE_DNS_SECRET_NAME&quot;,azure.resourceGroup=&quot;$AZURE_DNS_RESOURCE_GROUP&quot; \
    --set azure.tenantId=&quot;$AZURE_DNS_TENANT_ID&quot;,azure.subscriptionId=&quot;$AZURE_SUBSCRIPTION_ID&quot; \
    --set azure.aadClientId=&quot;$AZURE_DNS_CLIENT_ID&quot;,azure.aadClientSecret=&quot;$AZURE_DNS_CLIENT_SECRET&quot; \
    stable/external-dns
</pre></div>
</div>
<p>Key points:</p>
<ul class="simple">
<li><p>Watch for Kubernetes services, ingress, and the DNSEndpoint CRD published by the Elassandra operator when externalDns.enabled=true.</p></li>
<li><p>With <code class="docutils literal notranslate"><span class="pre">policy=sync</span></code>, we need to setup a txtPrefix per Kubernetes cluster in order to avoid update conflict between
clusters using the same DNS zone.</p></li>
</ul>
</div>
<div class="section" id="coredns">
<h3>CoreDNS<a class="headerlink" href="#coredns" title="Permalink to this headline">¶</a></h3>
<p>The Kubernetes CoreDNS is used for two reasons:</p>
<ul class="simple">
<li><p>Resolve DNS name of you DNS zone from inside the Kubernetes cluster using DNS forwarders.</p></li>
<li><p>Reverse resolution of the broadcast Elassandra public IP addresses to Kubernetes nodes private IP required by the AddressTranslator of the Cassandra driver.</p></li>
</ul>
<p>You can deploy the CodeDNS custom configuration with the strapdata coredns-forwarder HELM chart to basically install (or replace)
the coredns-custom configmap, and restart coreDNS pods.</p>
<p>If your Kubernetes nodes have the ExternalIP set (like GKE), prepare the coreDNS with this command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>HOST_ALIASES=$(kubectl get nodes -o custom-columns=&#39;INTERNAL-IP:.status.addresses[?(@.type==&quot;InternalIP&quot;)].address,EXTERNAL-IP:.status.addresses[?(@.type==&quot;ExternalIP&quot;)].address&#39; --no-headers |\
awk &#39;{ gsub(/\./,&quot;-&quot;,$2); printf(&quot;--set nodes.hosts[%d].name=%s,nodes.hosts[%d].value=%s &quot;,NR-1, $2, NR-1, $1); }&#39;)
</pre></div>
</div>
<p>If your Kubernetes nodes does not have the ExternalIP set (like AKS), public node IP address should be available through the custom label <code class="docutils literal notranslate"><span class="pre">kubernetes.strapdata.com/public-ip</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>HOST_ALIASES=$(kubectl get nodes -o custom-columns=&#39;INTERNAL-IP:.status.addresses[?(@.type==&quot;InternalIP&quot;)].address,PUBLIC-IP:.metadata.labels.kubernetes\.strapdata\.com/public-ip&#39; --no-headers |\
awk &#39;{ gsub(/\./,&quot;-&quot;,$2); printf(&quot;--set nodes.hosts[%d].name=%s,nodes.hosts[%d].value=%s &quot;,NR-1, $2, NR-1, $1); }&#39;)
</pre></div>
</div>
<p>Then configure the CoreDNS custom config with your DNS name servers, this is Azure name servers in the following example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>kubectl delete configmap --namespace kube-system coredns-custom
helm install $HELM_DEBUG --name coredns-forwarder --namespace kube-system \
    --set forwarders.domain=&quot;${DNS_DOMAIN}&quot; \
    --set forwarders.hosts[0]=&quot;40.90.4.8&quot; \
    --set forwarders.hosts[1]=&quot;64.4.48.8&quot; \
    --set forwarders.hosts[2]=&quot;13.107.24.8&quot; \
    --set forwarders.hosts[3]=&quot;13.107.160.8&quot; \
    --set nodes.domain=internal.strapdata.com \
    $HOST_ALIASES \
    strapdata/coredns-forwarder
</pre></div>
</div>
<p>Restart CoreDNS pods to reload our configuration, but this depends on coreDNS deployment labels !</p>
<p>On AKS:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">delete</span> <span class="n">pod</span> <span class="o">--</span><span class="n">namespace</span> <span class="n">kube</span><span class="o">-</span><span class="n">system</span> <span class="o">-</span><span class="n">l</span> <span class="n">k8s</span><span class="o">-</span><span class="n">app</span><span class="o">=</span><span class="n">kube</span><span class="o">-</span><span class="n">dns</span>
</pre></div>
</div>
<p>On GKE:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">delete</span> <span class="n">pod</span> <span class="o">--</span><span class="n">namespace</span> <span class="n">kube</span><span class="o">-</span><span class="n">system</span> <span class="o">-</span><span class="n">l</span> <span class="n">k8s</span><span class="o">-</span><span class="n">app</span><span class="o">=</span><span class="n">coredns</span>
</pre></div>
</div>
</div>
<div class="section" id="traefik">
<span id="traefik-setup"></span><h3>Traefik<a class="headerlink" href="#traefik" title="Permalink to this headline">¶</a></h3>
<p>Deploy a Traefik ingress controller in order to access to web user interfaces for the following components:</p>
<ul class="simple">
<li><p>Cassandra Reaper</p></li>
<li><p>Kibana</p></li>
<li><p>Prometheus Server</p></li>
<li><p>Prometheus Alert Manager</p></li>
<li><p>Grafana</p></li>
</ul>
<p>Here is simple Traefik deployment where TRAEFIK_FQDN=traefik-kube1.$DNS_DOMAIN:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>helm install --name traefik --namespace kube-system \
    --set rbac.enabled=true \
    --set dashboard.enabled=true,dashboard.domain=dashboard.${TRAEFIK_FQDN} \
    --set service.annotations.&quot;external-dns\.alpha\.kubernetes\.io/hostname&quot;=&quot;*.${TRAEFIK_FQDN}&quot; \
    stable/traefik
</pre></div>
</div>
<p>The externalDns annotation automatically publish the public IP of the Traefik ingress controller in our DNS zone.
To avoid conflict between Kubernetes cluster using the same DNS zone, the TRAEFIK_FQDN variable must
be the unique traefik FQDN in our DNS zone (example: traefik-kube1.my.domain.com)</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Of course, this Traefik setup is not secure, an it’s up to you to setup encryption and restrict access to those resources.</p>
</div>
</div>
<div class="section" id="elassandra-operator">
<h3>Elassandra Operator<a class="headerlink" href="#elassandra-operator" title="Permalink to this headline">¶</a></h3>
<p>Finally, install the Elassandra operator in the default namespace:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>helm install --namespace default --name elassop --wait $HELM_REPO/elassandra-operator
</pre></div>
</div>
</div>
</div>
<div class="section" id="multi-datacenter-setup">
<h2>Multi-datacenter setup<a class="headerlink" href="#multi-datacenter-setup" title="Permalink to this headline">¶</a></h2>
<div class="section" id="deploy-dc1-on-kube1">
<h3>Deploy dc1 on kube1<a class="headerlink" href="#deploy-dc1-on-kube1" title="Permalink to this headline">¶</a></h3>
<p>Deploy the first datacenter <strong>dc1</strong> of the Elassandra cluster <strong>cl1</strong> in the Kubernetes cluster <strong>kube1</strong>,
with Kibana and Cassandra Reaper available through the Traefik ingress controller.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>helm install --namespace default --name &quot;default-cl1-dc1&quot; \
    --set dataVolumeClaim.storageClassName=&quot;ssd-{zone}&quot; \
    --set cassandra.sslStoragePort=&quot;39000&quot; \
    --set cassandra.nativePort=&quot;39001&quot; \
    --set elasticsearch.httpPort=&quot;39002&quot; \
    --set elasticsearch.transportPort=&quot;39003&quot; \
    --set jvm.jmxPort=&quot;39004&quot; \
    --set jvm.jdb=&quot;39005&quot; \
    --set prometheus.port=&quot;39006&quot; \
    --set replicas=&quot;3&quot; \
    --set networking.hostNetworkEnabled=true \
    --set networking.externalDns.enabled=true \
    --set networking.externalDns.domain=${DNS_DOMAIN} \
    --set networking.externalDns.root=cl1-dc1 \
    --set kibana.enabled=&quot;true&quot;,kibana.spaces[0].ingressAnnotations.&quot;kubernetes\.io/ingress\.class&quot;=&quot;traefik&quot;,kibana.spaces[0].ingressSuffix=kibana.${TRAEFIK_FQDN} \
    --set reaper.enabled=&quot;true&quot;,reaper.ingressAnnotations.&quot;kubernetes\.io/ingress\.class&quot;=&quot;traefik&quot;,reaper.ingressHost=reaper.${TRAEFIK_FQDN} \
    --wait $HELM_REPO/elassandra-datacenter
</pre></div>
</div>
<p>Key points:</p>
<ul class="simple">
<li><p>The storageClass must exist in your Kubernetes cluster, default is the default storage class on Microsoft Azure.</p></li>
<li><p>Because <code class="docutils literal notranslate"><span class="pre">hostNetwork</span></code> is enabled, you need to properly choose TCP ports to avoid conflict on the Kubernetes nodes.</p></li>
<li><p>The env variable <strong>TRAEFIK_FQDN</strong> must be the public FQDN of your traefik deployment, traefik-kube1.$DNS_DOMAIN in our example.</p></li>
</ul>
<p>Wait for the datacenter <strong>dc1</strong> to be ready:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">edctl</span> <span class="n">watch</span><span class="o">-</span><span class="n">dc</span> <span class="o">--</span><span class="n">context</span> <span class="n">kube1</span> <span class="o">-</span><span class="n">n</span> <span class="n">elassandra</span><span class="o">-</span><span class="n">cl1</span><span class="o">-</span><span class="n">dc1</span> <span class="o">-</span><span class="n">ns</span> <span class="n">default</span> <span class="o">--</span><span class="n">health</span> <span class="n">GREEN</span>
</pre></div>
</div>
<p>Once your Elassandra datacenter is ready, check that you can reach the datacenter over the internet.
Get the Elassandra cluster root CA certificate and Cassandra admin password:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>kubectl get secret elassandra-cl1-ca-pub --context kube1 -n default -o jsonpath=&#39;{.data.cacert\.pem}&#39; | base64 -D &gt; cl1-cacert.pem
CASSANDRA_ADMIN_PASSWORD=$(kb get secret elassandra-cl1 --context kube1 -o jsonpath=&#39;{.data.cassandra\.admin_password}&#39; | base64 -D)
</pre></div>
</div>
<p>Check your Elassandra datacenter:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>SSL_CERTFILE=cl1-cacert.pem bin/cqlsh --ssl -u admin -p $CASSANDRA_ADMIN_PASSWORD cassandra-cl1-dc1-0-0.test.strapkube.com 39001
Connected to cl1 at cassandra-cl1-dc1-0-0.test.strapkube.com:39001.
[cqlsh 5.0.1 | Cassandra 3.11.6.1 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
admin@cqlsh&gt;
</pre></div>
</div>
<p>Check the Elasticsearch cluster status:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl -k --user admin:$CASSANDRA_ADMIN_PASSWORD &quot;https://cassandra-cl1-dc1-0-0.test.strapkube.com:39002/_cluster/state?pretty&quot;
{
  &quot;cluster_name&quot; : &quot;cl1&quot;,
  &quot;cluster_uuid&quot; : &quot;9754758d-bac6-4b92-0000-000000000000&quot;,
  &quot;version&quot; : 5,
  &quot;state_uuid&quot; : &quot;lXB_pTXvRnuLwE3jNOcaeQ&quot;,
  &quot;master_node&quot; : &quot;9754758d-bac6-4b92-0000-000000000000&quot;,
  &quot;blocks&quot; : { },
  &quot;nodes&quot; : {
    &quot;9754758d-bac6-4b92-0000-000000000000&quot; : {
      &quot;name&quot; : &quot;51.138.48.150&quot;,
      &quot;status&quot; : &quot;ALIVE&quot;,
      &quot;ephemeral_id&quot; : &quot;9754758d-bac6-4b92-0000-000000000000&quot;,
      &quot;transport_address&quot; : &quot;10.240.0.4:9300&quot;,
      &quot;attributes&quot; : {
        &quot;rack&quot; : &quot;0&quot;,
        &quot;dc&quot; : &quot;dc1&quot;
      }
    }
  },
  &quot;metadata&quot; : {
    &quot;version&quot; : 0,
    &quot;cluster_uuid&quot; : &quot;9754758d-bac6-4b92-0000-000000000000&quot;,
    &quot;templates&quot; : { },
    &quot;indices&quot; : { },
    &quot;index-graveyard&quot; : {
      &quot;tombstones&quot; : [ ]
    }
  },
  &quot;routing_table&quot; : {
    &quot;indices&quot; : { }
  },
  &quot;routing_nodes&quot; : {
    &quot;unassigned&quot; : [ ],
    &quot;nodes&quot; : {
      &quot;9754758d-bac6-4b92-0000-000000000000&quot; : [ ]
    }
  },
  &quot;snapshots&quot; : {
    &quot;snapshots&quot; : [ ]
  },
  &quot;restore&quot; : {
    &quot;snapshots&quot; : [ ]
  },
  &quot;snapshot_deletions&quot; : {
    &quot;snapshot_deletions&quot; : [ ]
  }
}
</pre></div>
</div>
<p>Once started, Kibana and Cassandra Reaper, Prometheus server, Prometheur alert manager and Grafana should be available in <strong>kube1</strong> at :</p>
<ul class="simple">
<li><p><a class="reference external" href="http://kibana-kibana.traefik-kube1.$DNS_DOMAIN/">http://kibana-kibana.traefik-kube1.$DNS_DOMAIN/</a></p></li>
<li><p><a class="reference external" href="http://reaper.traefik-kube1.$DNS_DOMAIN/webui">http://reaper.traefik-kube1.$DNS_DOMAIN/webui</a></p></li>
</ul>
<p>If the prometheus operator is deployed, you should get these user interfaces:
* <a class="reference external" href="http://prometheus.traefik-kube1.$DNS_DOMAIN/">http://prometheus.traefik-kube1.$DNS_DOMAIN/</a>
* <a class="reference external" href="http://alertmanager.traefik-kube1.$DNS_DOMAIN/">http://alertmanager.traefik-kube1.$DNS_DOMAIN/</a>
* <a class="reference external" href="http://grafana.traefik-kube1.$DNS_DOMAIN/login">http://grafana.traefik-kube1.$DNS_DOMAIN/login</a></p>
<p>For Kibana and Cassandra reaper, kibana and admin passwords are respectively stored in the Kubernetes secrets <strong>elassandra-cl1-kibana</strong> and <strong>elassandra-cl1-dc1-reaper</strong> in
the Elassandra datacenter namespace.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>KIBANA_PASSWORD=$(kb get secret elassandra-cl1-kibana --context kube1 -o jsonpath=&#39;{.data.kibana\.kibana_password}&#39; | base64 -D)
REAPER_ADMIN_PASSWORD=$(kb get secret elassandra-cl1-dc1-reaper --context kube1 -o jsonpath=&#39;{.data.password}&#39; | base64 -D)
</pre></div>
</div>
<p>Here is the Elasticsearch cluster state from the Kibana devtool:</p>
<img alt="_images/kibana-cluster-state.png" src="_images/kibana-cluster-state.png" />
<p>Here the Cassandra Reaper UI with our registered Cassandra cluster:</p>
<img alt="_images/reaper-cluster.png" src="_images/reaper-cluster.png" />
</div>
<div class="section" id="deploy-dc2-on-kube2">
<h3>Deploy dc2 on kube2<a class="headerlink" href="#deploy-dc2-on-kube2" title="Permalink to this headline">¶</a></h3>
<p>Once the Elassandra datacenter <strong>dc1</strong> is ready, you can deploy the datacenter <strong>dc2</strong> in the Kubernetes <strong>kube2</strong>.</p>
<p>First of all, copy the following Elassandra cluster secrets from the Kubernetes cluster <strong>kube1</strong> and
namespace <strong>default</strong>, into the Kubernetes cluster <strong>kube2</strong> namespace <strong>default</strong> (See the Security section for more information about these secrets):</p>
<ul class="simple">
<li><p>elassandra-cl1-dc1 (cluster passwords)</p></li>
<li><p>elassandra-cl1-dc1-ca-pub (cluster root CA)</p></li>
<li><p>elassandra-cl1-dc2-ca-key (cluster root CA key)</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>for s in elassandra-cl1 elassandra-cl1-ca-pub elassandra-cl1-ca-key; do
    kubectl get secret $s --context kube1 --export -n default -o yaml | kubectl apply --context gke_strapkube1_europe-west1_kube2 -n default -f -
done
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>These Elassandra cluster-wide secrets does not include any ownerReference &lt;<a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/">https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/</a>&gt;`_
and won’t be deleted when deleting the Elassandra datacenter because they could be used by another datacenter.
So, it’s up to you to properly delete these secrets when deleting an Elassandra cluster.</p>
</div>
<p>Deploy the datacenter <strong>dc2</strong> of the Elassandra cluster <strong>cl1</strong> in the Kubernetes cluster <strong>cluster2</strong>, with the following network settings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>helm install --namespace default --name &quot;default-cl1-dc2&quot; \
    --set dataVolumeClaim.storageClassName=&quot;ssd-{zone}&quot; \
    --set cassandra.sslStoragePort=&quot;39000&quot; \
    --set cassandra.nativePort=&quot;39001&quot; \
    --set elasticsearch.httpPort=&quot;39002&quot; \
    --set elasticsearch.transportPort=&quot;39003&quot; \
    --set jvm.jmxPort=&quot;39004&quot; \
    --set jvm.jdb=&quot;39005&quot; \
    --set prometheus.port=&quot;39006&quot; \
    --set replicas=&quot;1&quot; \
    --set cassandra.remoteSeeds[0]=cassandra-cl1-dc1-0-0.${DNS_DOMAIN} \
    --set networking.hostNetworkEnabled=true \
    --set networking.externalDns.enabled=true \
    --set networking.externalDns.domain=${DNS_DOMAIN} \
    --set networking.externalDns.root=cl1-dc2 \
    --set kibana.enabled=&quot;true&quot;,kibana.spaces[0].ingressAnnotations.&quot;kubernetes\.io/ingress\.class&quot;=&quot;traefik&quot;,kibana.spaces[0].ingressSuffix=kibana.${TRAEFIK_FQDN} \
    --set reaper.enabled=&quot;true&quot;,reaper.ingressAnnotations.&quot;kubernetes\.io/ingress\.class&quot;=&quot;traefik&quot;,reaper.ingressHost=reaper.${TRAEFIK_FQDN} \
    --wait $HELM_REPO/elassandra-datacenter
</pre></div>
</div>
<p>Key points :</p>
<ul class="simple">
<li><p>Storage class must be defined in the Kubernetes cluster to match <strong>ssd-{zone}</strong>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">cassandra.remoteSeeds</span></code> array must include the DNS name of a seed nodes in <strong>dc1</strong>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">networking.externalDns.root</span></code> must be different from the <strong>dc1</strong> to avoid DNS name conflict, and you can include namespace or whatever in your naming plan.</p></li>
<li><p>The <strong>TRAEFIK_FQDN</strong> env variable must point to the traefik public FQDN in the Kubernetes cluster <strong>kube2</strong>.</p></li>
</ul>
<p>Wait for the datacenter <strong>dc2</strong> to be ready:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">edctl</span> <span class="n">watch</span><span class="o">-</span><span class="n">dc</span> <span class="o">--</span><span class="n">context</span> <span class="n">kube2</span> <span class="o">-</span><span class="n">n</span> <span class="n">elassandra</span><span class="o">-</span><span class="n">cl1</span><span class="o">-</span><span class="n">dc2</span> <span class="o">-</span><span class="n">ns</span> <span class="n">default</span> <span class="o">--</span><span class="n">health</span> <span class="n">GREEN</span>
</pre></div>
</div>
<p>Before streaming the Cassandra data, you now need to adjust the replication factor for the following keyspaces:</p>
<ul class="simple">
<li><p>system_distributed</p></li>
<li><p>system_traces</p></li>
<li><p>system_auth</p></li>
<li><p>elastic_admin (if elasticsearch is enabled).</p></li>
<li><p>any user keyspace that you want to replicate in <strong>dc2</strong>, <em>foo</em> in the provided example.</p></li>
</ul>
<p>This is done with the following Elassandra task deployed on <strong>dc1</strong> (Kubernetes cluster <strong>cluster1</strong>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>cat &lt;&lt;EOF | kubectl apply --context kube1 -f -
apiVersion: elassandra.strapdata.com/v1beta1
kind: ElassandraTask
metadata:
  name: replication-add-$$
  namespace: default
spec:
  cluster: &quot;cl1&quot;
  datacenter: &quot;dc1&quot;
  replication:
    action: ADD
    dcName: &quot;dc2&quot;
    dcSize: 1
    replicationMap:
      elastic_admin: 1
      reaper_db: 1
      _kibana_1: 1
      foo: 1
EOF
edctl watch-task --context kube1 -n replication-add-$$ -ns default --phase SUCCEED
</pre></div>
</div>
<p>Then on <strong>dc2</strong>, run a rebuild task to stream data from <strong>dc1</strong> and wait for termination:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>cat &lt;&lt;EOF | kubectl apply --context kube2 -f -
apiVersion: elassandra.strapdata.com/v1beta1
kind: ElassandraTask
metadata:
  name: rebuild-dc2-$$
  namespace: default
spec:
  cluster: &quot;cl1&quot;
  datacenter: &quot;dc2&quot;
  rebuild:
    srcDcName: &quot;dc1&quot;
EOF
edctl watch-task --context kube2 -n rebuild-dc2-$$ -ns default --phase SUCCEED
</pre></div>
</div>
<p>If elasticsearch is enabled in <strong>dc2</strong>, you need to run restart Elassandra pods to update the Elasticsearch
cluster state since data have been populated by streaming data from <strong>dc1</strong>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">delete</span> <span class="n">pod</span> <span class="o">--</span><span class="n">namespace</span> <span class="n">default</span> <span class="o">-</span><span class="n">l</span> <span class="n">app</span><span class="o">=</span><span class="n">elassandra</span>
</pre></div>
</div>
<p>Finally, check the datacenter <strong>dc2</strong> is properly running on the Kubernetes cluster <strong>kube2</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>SSL_CERTFILE=cl1-cacert.pem bin/cqlsh --ssl -u admin -p $CASSANDRA_ADMIN_PASSWORD cassandra-cl1-dc2-0-0.test.strapkube.com 39001
Connected to cl1 at cassandra-cl2-dc1-0-0.test.strapkube.com:39001.
[cqlsh 5.0.1 | Cassandra 3.11.6.1 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
admin@cqlsh&gt;
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl -k --user admin:$CASSANDRA_ADMIN_PASSWORD &quot;https://cassandra-cl1-dc2-0-0.test.strapkube.com:39002/_cluster/state?pretty&quot;
</pre></div>
</div>
</div>
</div>
<div class="section" id="cleaning-up">
<h2>Cleaning up<a class="headerlink" href="#cleaning-up" title="Permalink to this headline">¶</a></h2>
<p>Delete an Elassandra datacenter:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">helm</span> <span class="n">delete</span> <span class="o">--</span><span class="n">purge</span> <span class="n">elassandra</span><span class="o">-</span><span class="n">cl1</span><span class="o">-</span><span class="n">dc1</span>
</pre></div>
</div>
<p>Undeploy the Elassandra operator and remove CRDs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">helm</span> <span class="n">delete</span> <span class="o">--</span><span class="n">purge</span> <span class="n">elassandra</span><span class="o">-</span><span class="n">operator</span>
<span class="n">kubectl</span> <span class="n">delete</span> <span class="n">crd</span> <span class="n">elassandradatacenters</span><span class="o">.</span><span class="n">elassandra</span><span class="o">.</span><span class="n">strapdata</span><span class="o">.</span><span class="n">com</span> <span class="n">elassandratasks</span><span class="o">.</span><span class="n">elassandra</span><span class="o">.</span><span class="n">strapdata</span><span class="o">.</span><span class="n">com</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="security.html" class="btn btn-neutral float-left" title="Security" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Strapdata

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>