Configuration
-------------

Elassandra configuration is generated by concatenating files from the following configuration sub-directories in /etc/cassandra:

* cassandra-env.sh.d
* cassandra.yaml.d
* elasticsearch.yml.d
* jvm.options.d

Files are loaded in alphanumeric order, so the last file overrides previous settings.

Elassandra DataCenter
.....................

Cassandra Seeds
_______________

The Elassandra operator use a custom Cassandra seed provider using the following 3 parameters :

.. cssclass:: table-bordered

+----------------+----------------+-----------------------------------------------------------------------------+
| Parameter      | Env variable   | Description                                                                 |
+================+================+=============================================================================+
| seeds          | SEEDS          | Local seed addresses or DNS hostname.                                       |
+----------------+----------------------------------------------------------------------------------------------+
| remote_seeds   | REMOTE_SEEDS   | Remote datacenters seed addresses or DNS names.                             |
+----------------+----------------------------------------------------------------------------------------------+
| remote_seeders | REMOTE_SEEDERS | Remote elassandra operator web service URL providing remote seed addresses. |
+----------------+----------------+-----------------------------------------------------------------------------+

Empty parameters are replaced by the associated env variable if available.

Finally, if no seed addresses is found from theses parameters, the seed provider automatically add the broadcast address
to bootstrap the node.

.. TIP::

    The Elassandra operator expose one seed address per rack on the HTTP endpoint ``/seeds/{namespace}/{clusterName}/{datacenterName}``.
    This endpoint can be exposed to a remote Kubernetes cluster hosting a remote Elassandra datacenter by using the
    appropriate Kubernetes service.

User configuration
..................

You can add you own configuration file to Elassandra nodes by defining a Kubernetes configmap where each key is mapped to a file.
Here is an example to customize Cassandra settings from the cassandra.yaml file:

1. Create and deploy your user-config map:

.. code::

    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: elassandra-cl1-dc1-user-config
      namespace: default
      labels:
        app: elassandra
        cluster: cl1
        datacenter: dc1
        parent: elassandra-cl1-dc1
    data:
      cassandra_yaml_d_user_config_overrides_yaml: |
        memtable_cleanup_threshold: 0.12

2. Patch the elassandraDatacenter CRD to map the user-config map to cassandra.yaml.d/009-user_config_overrides.yaml:

.. code::

    kubectl patch elassandradatacenter elassandra-cl1-dc1 --type merge --patch '{"spec":
        {"userConfigMapVolumeSource":
            {"name":"elassandra-cl1-dc1-user-config","items":[
                {"key":"cassandra_yaml_d_user_config_overrides_yaml","path":"cassandra.yaml.d/009-user_config_overrides.yaml"},
                {"key":"logback.xml","path":"logback.xml"}]
            }
        }
    }'

3. The Elassandra operator detects the CRD change and update per rack statefulsets.


.. CAUTION::

    If you patch the CRD with a wrong schema, the elassandra operator won't be able to parse and process it until you fix it.

Resources configuration
.......................

You can adjust CPU and Memory needs of your Elassandra nodes by updating the CRD elassandradatacenter as shown here:

.. code::

    kubectl patch elassandradatacenter elassandra-cl1-dc1 --type merge --patch '{"spec":{"resources":{"limits":{"memory":"4Gi"}}}}'

Resources entry may receive "limits" and/or "requests" quantity description as describe in the [k8s documentation](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/).

.. code::

    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 1000m
        memory: 2Gi


Pod affinity
.......................

You can define the the [NodeAffinity](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity) for the elassandra pods using the "nodeAffinityPolicy" attribute of the DatacenterSpec.

.. code::

    kubectl patch elassandradatacenter elassandra-cl1-dc1 --type merge --patch '{"spec":{"nodeAffinityPolicy": "STRICT"}}'

Possible values are :
* STRICT : schedule elassandra pods only on nodes in the matching the failure-domain.beta.kubernetes.io/zone label (default value)
* SLACK : schedule elassandra pods preferably on nodes in the matching the failure-domain.beta.kubernetes.io/zone label

Data Volume Claim
........................

To specify the persistence characteristics for each Elassandra node, you can describe a [PersistentVolumeClaimSpec](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#persistentvolumeclaimspec-v1-core) as "dataVolumeClaim" value.

.. code::

    dataVolumeClaim:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 500Mi

