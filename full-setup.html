

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Full Setup Example &mdash; Elassandra Operator Documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://media.readthedocs.org/css/sphinx_rtd_theme.css" type="text/css" />
  <link rel="stylesheet" href="https://media.readthedocs.org/css/readthedocs-doc-embed.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/custom.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Compute resources" href="compute-resources.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> Elassandra-Operator
          

          
            
            <img src="_static/elassandra-operator.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="helm-setup.html">HELM setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator-setup.html">Operator setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="provisioning.html">Provision a Datacenter</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced-services.html">Advanced services</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="operations.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="compute-resources.html">Compute resources</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Full Setup Example</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#externaldns">ExternalDNS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#coredns">CoreDNS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#traefik">Traefik</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prometheus-operator">Prometheus operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#aks-setup">AKS Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gke-setup">GKE Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#coredns-installation">CoreDNS installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#webhook">Webhook</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#elassandra-datacenter">Elassandra datacenter</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Elassandra-Operator</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Full Setup Example</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/full-setup.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="full-setup-example">
<h1>Full Setup Example<a class="headerlink" href="#full-setup-example" title="Permalink to this headline">¶</a></h1>
<div class="section" id="externaldns">
<h2>ExternalDNS<a class="headerlink" href="#externaldns" title="Permalink to this headline">¶</a></h2>
<p>The ExternalDNS is used to automatically update your DNS zone. In the following setup, we will use
a DNS zone hosted on Azure, but you can use any other DNS provider supported by External DNS.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>helm install $HELM_DEBUG --name my-externaldns --namespace default \
--set logLevel=&quot;debug&quot; \
--set rbac.create=true \
--set policy=&quot;sync&quot;,txtPrefix=$(kubectl config current-context)\
--set sources[0]=&quot;service&quot;,sources[1]=&quot;ingress&quot;,sources[2]=&quot;crd&quot; \
--set crd.create=true,crd.apiversion=&quot;externaldns.k8s.io/v1alpha1&quot;,crd.kind=&quot;DNSEndpoint&quot; \
--set provider=&quot;azure&quot; \
--set azure.secretName=&quot;$AZURE_DNS_SECRET_NAME&quot;,azure.resourceGroup=&quot;$AZURE_DNS_RESOURCE_GROUP&quot; \
--set azure.tenantId=&quot;$AZURE_DNS_TENANT_ID&quot;,azure.subscriptionId=&quot;$AZURE_SUBSCRIPTION_ID&quot; \
--set azure.aadClientId=&quot;$AZURE_DNS_CLIENT_ID&quot;,azure.aadClientSecret=&quot;$AZURE_DNS_CLIENT_SECRET&quot; \
stable/external-dns
</pre></div>
</div>
<p>Key points:</p>
<ul class="simple">
<li><p>Watch for Kubernetes services, ingress, and the DNSEndpoint CRD published by the Elassandra operator when externalDns.enabled=true.</p></li>
<li><p>With policy=sync, we need to setup a txtPrefix per Kubernetes cluster in order to avoid update conflict between
clusters using the same DNS zone.</p></li>
</ul>
</div>
<div class="section" id="coredns">
<h2>CoreDNS<a class="headerlink" href="#coredns" title="Permalink to this headline">¶</a></h2>
<p>The Kubernetes CoreDNS is used for two reasons:</p>
<ul class="simple">
<li><p>Resolve DNS name of you DNZ zone from inside our Kubernetes cluster using DNS forwarders.</p></li>
<li><p>Reverse resolution of the broadcast Elassandra public IP addresses to Kubernetes nodes names.</p></li>
</ul>
<p>You can deploy the CodeDNS custom configuration with the coredns-forwarder HELM chart to basically replace the coredns-custom configmap,
and restart coreDNS pods.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>kubectl delete configmap --namespace kube-system coredns-custom
helm install $HELM_DEBUG --name coredns-forwarder --namespace kube-system \
    --set forwarders.domain=&quot;${DNS_DOMAIN}&quot; \
    --set forwarders.hosts[0]=&quot;&lt;forwarder1-ip-address&gt;&quot; \
    --set forwarders.hosts[1]=&quot;&lt;forwarder1-ip-address&gt;&quot; \
    --set nodes.hosts[0].name=&quot;&lt;node-0&gt;&quot;,nodes.hosts[0].value=&quot;&lt;node0-public-ip&gt;&quot; \
    strapdata/coredns-forwarder
kubectl delete pod --namespace kube-system -l k8s-app=kube-dns
</pre></div>
</div>
</div>
<div class="section" id="traefik">
<h2>Traefik<a class="headerlink" href="#traefik" title="Permalink to this headline">¶</a></h2>
<p>Deploy a Traefik ingress controller in order to access to web user interface of theo following components:</p>
<ul class="simple">
<li><p>Cassandra Reaper</p></li>
<li><p>Kibana</p></li>
<li><p>Prometheus Server</p></li>
<li><p>Prometheus Alert Manager</p></li>
<li><p>Grafana</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>helm install $HELM_DEBUG --name traefik --namespace kube-system \
--set rbac.enabled=true,debug.enabled=true \
--set dashboard.enabled=true,dashboard.domain=dashboard.${1:-$$TRAFIK_FQDN} \
--set service.annotations.&quot;external-dns\.alpha\.kubernetes\.io/hostname&quot;=&quot;*.${1:-$$TRAFIK_FQDN}&quot; \
stable/traefik
</pre></div>
</div>
<p>The externalDns annotation automatically publish the public IP of the traefik ingress controller in our DNS zone.
To avoid conflict between Kubernetes cluster using the same DNS zone, the TRAFIK_FQDN variable must
be the unique traefik FQDN in our DNS zone (example: traefik-dc1.my.domain.com)</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Of course, this traefik setup is not secure, an it’s up to you to setup encryption and restrict access to those resources.</p>
</div>
</div>
<div class="section" id="prometheus-operator">
<h2>Prometheus operator<a class="headerlink" href="#prometheus-operator" title="Permalink to this headline">¶</a></h2>
<p>To monitor your Kubernetes cluster, you can deploy the Prometheus operator, with the traefik ingress configured as shown bellow.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>helm install $HELM_DEBUG --name my-promop \
--set prometheus.ingress.enabled=true,prometheus.ingress.hosts[0]=&quot;prometheus.${TRAEFIK_FQDN}&quot;,prometheus.ingress.annotations.&quot;kubernetes\.io/ingress\.class&quot;=&quot;traefik&quot; \
--set alertmanager.ingress.enabled=true,alertmanager.ingress.hosts[0]=&quot;alertmanager.${TRAEFIK_FQDN}&quot;,alertmanager.ingress.annotations.&quot;kubernetes\.io/ingress\.class&quot;=&quot;traefik&quot; \
--set grafana.ingress.enabled=true,grafana.ingress.hosts[0]=&quot;grafana.${TRAEFIK_FQDN}&quot;,grafana.ingress.annotations.&quot;kubernetes\.io/ingress\.class&quot;=&quot;traefik&quot; \
--set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
-f integ-test/prometheus-operator-values.yaml \
stable/prometheus-operator
</pre></div>
</div>
<p>The file <strong>integ-test/prometheus-operator-values</strong> defines the following scarp config
to properly scrap pods having the prometheus annotation (The Elassandra operator does not deploy ServiceMonitor CRDs):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>prometheus:
  prometheusSpec:
    additionalScrapeConfigs:
      - job_name: &#39;kubernetes-pods&#39;
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: instance
</pre></div>
</div>
</div>
<div class="section" id="aks-setup">
<h2>AKS Setup<a class="headerlink" href="#aks-setup" title="Permalink to this headline">¶</a></h2>
<p>By default, the AKS does not allow to add public IP addresses on Kubernetes nodes.
The trick is to remove the kubernetes LoadBalancer, and create a new one with a Standard SKU.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>AKS_RG_NAME=$(az resource show --namespace Microsoft.ContainerService --resource-type managedClusters -g $RESOURCE_GROUP_NAME -n $K8S_CLUSTER_NAME | jq -r .properties.nodeResourceGroup)
az network lb delete --name kubernetes -g $AKS_RG_NAME
az network lb create --name kubernetes -g $AKS_RG_NAME --sku Standard
</pre></div>
</div>
<p>Then, create and add a public IP to each Kubernetes nodes, and set the label kuberenetes.strapdata.com/public-ip with the node’s public IP.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>add_public_ip() {
   AKS_RG_NAME=$(az resource show --namespace Microsoft.ContainerService --resource-type managedClusters -g $RESOURCE_GROUP_NAME -n &quot;${1}&quot; | jq -r .properties.nodeResourceGroup)
   AKS_NODE=$(az vm list --resource-group $AKS_RG_NAME | jq -r &quot;.[$2] .name&quot;)
   #az network nic ip-config list --nic-name &quot;${AKS_NODE::-2}-nic-0&quot; -g $AKS_RG_NAME

   # create a new public IP
   az network public-ip create -g $AKS_RG_NAME --name &quot;${1}-ip$2&quot; --dns-name &quot;${1}-pub${2}&quot; --sku Standard
   az network nic ip-config update -g $AKS_RG_NAME --nic-name &quot;${AKS_NODE::-2}-nic-0&quot; --name ipconfig1 --public-ip-address &quot;${1}-ip$2&quot;

   PUBLIC_IP=$(az network public-ip show -g $AKS_RG_NAME --name &quot;${1}-ip$2&quot; | jq -r &quot;.ipAddress&quot;)
   kubectl label nodes --overwrite $AKS_NODE kubernetes.strapdata.com/public-ip=$PUBLIC_IP
}
add_public_ip ${1:-$K8S_CLUSTER_NAME} 0
</pre></div>
</div>
<p>As the result, you should have kubernetes nodes with the following labels:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">get</span> <span class="n">nodes</span> <span class="o">-</span><span class="n">o</span> <span class="n">wide</span> <span class="o">-</span><span class="n">L</span> <span class="n">failure</span><span class="o">-</span><span class="n">domain</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">kubernetes</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">zone</span><span class="p">,</span><span class="n">kubernetes</span><span class="o">.</span><span class="n">strapdata</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">public</span><span class="o">-</span><span class="n">ip</span>
<span class="n">NAME</span>                       <span class="n">STATUS</span>   <span class="n">ROLES</span>   <span class="n">AGE</span>   <span class="n">VERSION</span>    <span class="n">INTERNAL</span><span class="o">-</span><span class="n">IP</span>   <span class="n">EXTERNAL</span><span class="o">-</span><span class="n">IP</span>   <span class="n">OS</span><span class="o">-</span><span class="n">IMAGE</span>             <span class="n">KERNEL</span><span class="o">-</span><span class="n">VERSION</span>      <span class="n">CONTAINER</span><span class="o">-</span><span class="n">RUNTIME</span>       <span class="n">ZONE</span>   <span class="n">PUBLIC</span><span class="o">-</span><span class="n">IP</span>
<span class="n">aks</span><span class="o">-</span><span class="n">nodepool1</span><span class="o">-</span><span class="mi">36354689</span><span class="o">-</span><span class="mi">0</span>   <span class="n">Ready</span>    <span class="n">agent</span>   <span class="mi">26</span><span class="n">h</span>   <span class="n">v1</span><span class="o">.</span><span class="mf">15.11</span>   <span class="mf">10.240</span><span class="o">.</span><span class="mf">0.4</span>    <span class="o">&lt;</span><span class="n">none</span><span class="o">&gt;</span>        <span class="n">Ubuntu</span> <span class="mf">16.04</span><span class="o">.</span><span class="mi">6</span> <span class="n">LTS</span>   <span class="mf">4.15</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="mi">1083</span><span class="o">-</span><span class="n">azure</span>   <span class="n">docker</span><span class="p">:</span><span class="o">//</span><span class="mf">3.0</span><span class="o">.</span><span class="mi">10</span><span class="o">+</span><span class="n">azure</span>   <span class="mi">0</span>      <span class="mf">20.54</span><span class="o">.</span><span class="mf">40.201</span>
</pre></div>
</div>
<p>To connect two Elassandra datacenters running in distinct Kubernetes clusters, you now need to configure the CoreDNS to
resolve DNS names in your DNS zone and revers lookup public IP addresses of Kubernetes nodes to Kubernetes nodes name.
See the CoreDNS setup.</p>
<p>Finally, you may need to authorize inbound Elassandra connections on the following TCP ports:</p>
<ul class="simple">
<li><p>Cassandra storage port (usually 7000 or 7001) for internode connections</p></li>
<li><p>Cassandra native CQL port (usually 9042) for client to node connections.</p></li>
<li><p>Elasticsearch HTTP port (usually 9200) for the Elasticsearch REST API.</p></li>
</ul>
<p>Assuming you deploy an Elassandra datacenter using ports 39000, 39001, and 39002 exposed to the internet, with no source IP address restrictions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>AKS_RG_NAME=$(az resource show --namespace Microsoft.ContainerService --resource-type managedClusters -g $RESOURCE_GROUP_NAME -n &quot;${1}&quot; | jq -r .properties.nodeResourceGroup)
NSG_NAME=$(az network nsg list -g $AKS_RG_NAME | jq -r .[0].name)
az network nsg rule create \
    --resource-group $AKS_RG_NAME \
    --nsg-name $NSG_NAME \
    --name elassandra_inbound \
    --description &quot;Elassandra inbound rule&quot; \
    --priority 2000 \
    --access Allow \
    --source-address-prefixes 0.0.0.0 \
    --protocol Tcp \
    --direction Inbound \
    --destination-address-prefixes &#39;*&#39; \
    --destination-port-ranges 39000 39001 39002
</pre></div>
</div>
<p>Your Kubernetes cluster is now ready to deploy an Elassandra datacenter accessible from the internet world.</p>
</div>
<div class="section" id="gke-setup">
<h2>GKE Setup<a class="headerlink" href="#gke-setup" title="Permalink to this headline">¶</a></h2>
<div class="section" id="coredns-installation">
<h3>CoreDNS installation<a class="headerlink" href="#coredns-installation" title="Permalink to this headline">¶</a></h3>
<p>GKE is provided with KubeDns by default, which does not allows to configure host aliases.
You should install CoreDNS and scale to 0 replica the KubeDNS as shown bellow:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wget</span> <span class="o">-</span><span class="n">O</span> <span class="o">-</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="o">.</span><span class="n">githubusercontent</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">coredns</span><span class="o">/</span><span class="n">deployment</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">kubernetes</span><span class="o">/</span><span class="n">deploy</span><span class="o">.</span><span class="n">sh</span> <span class="o">|</span> <span class="n">bash</span> <span class="o">|</span> <span class="n">kubectl</span> <span class="n">apply</span> <span class="o">-</span><span class="n">f</span> <span class="o">-</span>
<span class="n">kubectl</span> <span class="n">scale</span> <span class="n">deployment</span> <span class="o">--</span><span class="n">replicas</span><span class="o">=</span><span class="mi">0</span> <span class="n">kube</span><span class="o">-</span><span class="n">dns</span> <span class="o">--</span><span class="n">namespace</span><span class="o">=</span><span class="n">kube</span><span class="o">-</span><span class="n">system</span>
<span class="n">kubectl</span> <span class="n">scale</span> <span class="n">deployment</span> <span class="o">--</span><span class="n">replicas</span><span class="o">=</span><span class="mi">0</span> <span class="n">kube</span><span class="o">-</span><span class="n">dns</span><span class="o">-</span><span class="n">autoscaler</span> <span class="o">--</span><span class="n">namespace</span><span class="o">=</span><span class="n">kube</span><span class="o">-</span><span class="n">system</span>
</pre></div>
</div>
</div>
<div class="section" id="webhook">
<h3>Webhook<a class="headerlink" href="#webhook" title="Permalink to this headline">¶</a></h3>
<p>When Google configure the control plane for private clusters, they automatically configure VPC peering between your Kubernetes cluster’s network and a separate Google managed project.
In order to restrict what Google are able to access within your cluster, the firewall rules configured restrict access to your Kubernetes pods.
This means that in order to use the webhook component with a GKE private cluster, you must configure an additional firewall rule
to allow the GKE control plane access to your webhook pod.</p>
<p>You can read more information on how to add firewall rules for the GKE control plane nodes in the GKE docs</p>
<p>Alternatively, you can disable the hooks by setting webhookEnabled=false in your datacenter spec.</p>
</div>
</div>
<div class="section" id="elassandra-datacenter">
<h2>Elassandra datacenter<a class="headerlink" href="#elassandra-datacenter" title="Permalink to this headline">¶</a></h2>
<p>On cluster1:</p>
<p>install_elassandra_datacenter default cl1 dc1 1 “networking.hostNetworkEnabled=true,networking.externalDns.enabled=true,networking.externalDns.domain=test.strapkube.com,networking.externalDns.root=cl1-dc1”</p>
<p>Deploy the first datacenter dc1, with the following settings:</p>
<p>On cluster2:</p>
<p>install_elassandra_datacenter default cl1 dc2 1 “networking.hostNetworkEnabled=true,networking.externalDns.enabled=true,networking.externalDns.domain=test.strapkube.com,networking.externalDns.root=cl1-dc2,cassandra.remoteSeeds[0]=cassandra-cl1-dc1-0-0.test.strapkube.com”</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="compute-resources.html" class="btn btn-neutral float-left" title="Compute resources" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Strapdata

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>